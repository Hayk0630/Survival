# In-Hospital Mortality Prediction

This project predicts whether a patient will die during hospital stay (`In-hospital_death`) from clinical features.

## Objective
Build a practical ML pipeline with strong recall while still considering precision.

Primary score used in experiments:

- $0.8 \times Recall + 0.2 \times Precision$

## Main model choice
The final production model is `GradientBoostingClassifier` with:

- `n_estimators=100`
- `learning_rate=0.1`
- `max_depth=3`
- `min_samples_leaf=1`
- decision threshold: `0.20`

## Dataset
- file: `Survival_dataset.csv`
- target: `In-hospital_death`

## Models tried
- Logistic Regression
- Random Forest
- Gradient Boosting (main)
- XGBoost

Metrics for tried models are exported to `tried_models_metrics.csv` (one row per model).

## Comparison figure
![Model comparison](comparison_table.png)

## Project structure
- `preprocessor.py` -> `Preprocessor` class with `fit`, `transform`, `fit_transform`, `save`, `load`
- `model.py` -> `Model` class with `fit`, `predict`, `predict_proba`, `save`, `load`
- `run_pipeline.py` -> `Pipeline` class with `run(data_path, test=False)` and CLI
- `run.py` -> compatibility wrapper that delegates to `run_pipeline.py`

## How to run

### 1) Training mode
Input must include both features and label (`In-hospital_death`).

```bash
python run_pipeline.py --data_path Survival_dataset.csv
```

Training mode saves:
- `preprocessor.joblib`
- `model.joblib`
- `tried_models_metrics.csv`

### 2) Testing mode
Input should contain only features (no label required).

```bash
python run_pipeline.py --data_path <path_to_test_csv> --test
```

Testing mode saves:
- `predictions.json`

`predictions.json` contains two keys:
- `predict_probas`: predicted probability for each row
- `threshold`: recommended threshold (probability >= threshold => class 1)

## Reproducibility note
If you run training mode again on the same training data and random seed, `preprocessor.joblib` and `model.joblib` are regenerated by the pipeline and can be reused in testing mode without retraining.
